> Spark:
	- Batch and real-time data processing system.
	
> Spark Modes of Deployment:
	- Local  Mode: Runs on Single machine.
	- Cluster Mode: Set of multiple machines.
	
> repl: Interactive shell of Spark;

> SparkContxt: Entry point in the Spark Cluster. Way to connect in the Spark World.

> RDD(Resilient Distributed Dataset): Base data model for . It is a distributed Dataset. It is a distributed data structure of spark. 
> RDD is a collection of the data in the blocks known as Partitions created by Spark. The blocks of spark collects the data from the blocks of Hadoop.
	
> RDD pratitions are created on the memory of the DataNodes where the blocks of Data are created.

# export PYSPARK_DRIVER_PYTHON="jupyter"
# export PYSPARK_DRIVER_PYTHON_OPTS="notebook"

> PySpark SQL is spark library for structured data. Provides more info. about structured data and computation.

> PySpark Data Frame is an immutable distributed collection of data with name columns.

> Parquet file can store the data with metadata. (data with respective schema).

> Run Spark Job on hadoop cluster. (using cluster mode of deployment)
	# spark-submit --master yarn --deploy-mode cluster --driver-memory 2g --num-executors 2 --executor-memory 1g duplicates.py
	# yarn logs -applicationId <applicationId> : To see logs (output) of the Job on cluster side.
	
> Run Spark Job on hadoop cluster. (using client mode of deployment)
	# spark-submit --master yarn --deploy-mode client --driver-memory 2g --num-executors 2 --executor-memory 1g duplicates.py
	
> On Cluster mode of Deployment, the application runs on cluster side and the output is displayed on the cluster side.

> On Client mode of Deployment, the application runs on cluster side and the output is displayed on the client side.