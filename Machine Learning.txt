> Artificial Intelligence : Technology that can do its task without any human intervention(action).\

> Machine learning: It provides the statistical tools for analyze and forecast or predict the data.

> Deep Learning: Subset of ML. It is used to mimic the human brian i.e it learns from the data just like human brain.

> Main job of Data Scientist is to deal with all the above technology to develop an AI application.

> How to learn and understand the working of any Algorithm:
    1. What problem the algorithm solve?
    2. Geometric Intuition: Understand the solving answers using diagrams.
    3. Mathematical Intuition: Understand the mathematics behind the working of Algorithms. 

> Types of ML Algorithms:
    - Supervised ML: Predict the values of dependant variable from independant variable. It gives the data of dependant and independant
                    variable to train the model. And when a new value comes, the model predicts the result of new value based on the results of old values. 
        -> Types of Supervised ML: 
            1. Classification: The dependant feature(variable or column) contains categorical values.
                eg. Pass/Fail -> Binary Classification Problem.
                    Predicting the type of animal from data i.e Lion, Tiger, Dog, Cat -> Multiclass classification.

            2. Regression: The dependant feature(variable or column) contains numerical or continuous values.
                eg. House Prices prediction by its location.
                    Prediction of Salary by Designation and Experience.

    - Unsupervised ML: There is no output in this type of ML Algo. The dataset if divided into groups called CLUSTERS. Unsupervised learning models don't need supervision while training data sets, making it an ideal ML technique for discovering patterns, groupings and differences in unstructured data.

> Parameters: They are the arguments which are used to control the training aspects of the algorithm. Parameters are passed by default internally by the algorithm itself if we do not specify any of them.

> Hyperparameter : They are the user-defined arguments which are used to control the training criteria of the algorithm. They are expicitly passed by user while writing the code.
    eg. max_depth of the tree: [3,4,5,6,7] --> It will calculate the output for all the depths of the tree.

> GridSearchCV : It is a technique which enable us to pass multiple hyperparameter and chooses the best score and best parameter from it. Finds the optimal parameter values from a given set of parameters in a grid.

------------------------------------------------------------------------------------------------------------------------------------------

> ML Algorithms:

    -> Linear Regression: 
        - Use to identify the best fit line with minimal error the help of calculating the means of the errors of all the datapoints.
        - Predicts a value of a variable on the basis of various dependant variable.
        - Residual error: Difference between actual value(Value on the best fit line) and predicted Value of a record in the dataset.
        - Best fit line is a line whcih tells you about the predicted values of Y variable with respect to X variable.
        - Equation of straight line:
            y = mx + c 
                where m -> slope of line. It means change in value of Y variable when X variable is changed by 1 unit.


    -> Ridge Regression:
        - If the best fit line passes through all the points of the training data, the accracy of model is 100%.
        - But when a new point arrives as a testing data, the best fit line does not pass from all the points.
        - Hence, the model seems to be overfitted.
        - Overfitting:
            -> When, Model performs well with training data (Model accuracy is good i.e passes through all points) and fails to perform with test data ------> Low Bias , High Variance.
        - Underfitting:    
            -> When, Model fails to performs well with training data (Model accuracy is bad i.e. doesnt passes through all points) and fails to perform with test data ------> High Bias , High Variance.
        - Ridge Regression is used to avoid the overfitting of the algorithm by adding the product of hyperparameter (lambda) and slope square to the equation.
        - lambda hyperparameter specifies that how fast the steepness of line must be made lesser or higher.
        - Main aim of Ridge Regression and lasso Regression is to make cost function as low as possible but not zero to avoid overfitting.

    -> Lasso Regression:
        - Lasso Regression is used to avoid the overfitting of the algorithm by adding the product of hyperparameter (lambda) and mod of slope to the equation.
        - It helps to avoid the overfitting of algorithm and also helps for feature selection.

	
    -> Naive Bayes Algorithm: 
        - It is a Classification Algorithm.
        - It predicts the probability of independant variable with the help of probabilities of dependant variable.
        - It uses Bayes Theorem to calculate the probability of response variable i.e. given by:
        - If the probability of dependant variable > 0.5 -----> True, else ---------> False

            P(B|A) = (P(B) * P(A|B)) / P(A) 

        - In case of dataset columns:

            P(y|X1,X2,X3 ... Xn) = (P(Y) * P(X1)*P(X2)*P(X3) ... P*(Xn)) / P(X1)*P(X2)*P(X3) ... P*(Xn)

                where y = Dependant variable (variable to be predicted)
                    X1,X2,X3 ... Xn : independant variables.

        - e.g.
            P(Yes|Xi) = 0.13    &   P(No|Xi) = 0.05         ... Xi is one of the column of dataset.
            Hence, the predicted value for Y will be = Yes.

    -> K- Nearest Neighbour: 
        - Standardization is required.
        - Gets affected by outliers.

    -> DecisionTreeClassification:
        - It is a classification algorithm which makes a tree by splits the values in column by the category. 
            eg. Weather column is splitted into 3 categoryies Sunny, Overcast, Rain. 
        - Pure Split:
            - The possibility of the leaf node is either all YES or all NO. (probability of any 1 category is 100%).
                eg. Chances to play tennis in sunny weather is 4 Yes / 0 No.
        - Pure Split Node will never be splitted further.
        - Impure Split:
            - The possibility of the leaf node is distributed. (probability of any 1 category is 70% and 2 category is 30%).
        - In case of Impure Split, the algorithm takes another feature for splitting after the values of 1st column gets over for splitting.
            eg. possibility to play tennis in sunny weather is 2 Yes/3No. Hence, the temperature feature will be considered after the sunny weather values are not remaining.

        - Entropy: 
            - A measure of disorder or impurity in a node. Entropy is an information theory metric that measures the impurity or uncertainty in a group of observations.
        - Entropy will be always in between 0 to 1.
            - If Entropy = 0 (Pure Split). 
                Entropy = 1 (Impure Split).
        - Entropy of Leaf Node = 0 
        - If the Entropy is > 0 , then split the column further.
        - Information gain: 
            - Use to decide which column to be considered for splitting. The column with larger Information Gain will be considered first for splitting.
            eg. Info. Gain of Feature 1 = 0.049
            Info. Gain of Feature 1 = 0.051             -------> Splitting will start from Feature 2.
        - Information Gain can be calculated by calculating the entropy of every column by considering all the possible splits of the column and add the value of entropy of all splits.

        - Gini impurity:
            - Gini impurity is a function that determines how well a decision tree was split.  Basically, it helps us to determine which splitter is best so that we can build a pure decision tree. 
            - Gini impurity ranges values from 0 to 0.5.
            - It is similar to Entropy.
            - It is faster than Entropy. Hence, Gini Impurity is used tocalculate the impurity. 

        - What if, The features contains all continuous values.
            - It will calculate information gain of every value in the feature and the value with highest information gain, it will be considered as a root node.


	- DecisionTreeRegression:
        - How a column is selected for splitting? 
            - Initially, the Mean of Output variable is calculated.
            - Later, the MEAN_SQUARED_ERROR is used instead of Entropy and Gini Impurity.
            - The mean value is assigned to Feature 1 and it computes MEAN_SQUARED_ERROR of the column. Then it will split the values. 
            - As the mean value gets reduced, closer to the leaf node we go.     
    
    
    -> Ensemble techniques:
        - Works for both Classification and Regression.
        1.Voting:
			- Can Apply Multiple algorithms on a single dataset and predict the values of response variable. We consider the predicted value which occurs majority number of times.
				eg. Value of dependant variable using:
					1. Logistic Regression: 1
					2. DecisionTreeClassifier: 0
					3. LDA : 0 
					Predicted value = 0
			- Can also take probabilities instead of 0 & 1 and calculate the mean of probabilities.
			- When we predict the values in 0 and 1 -------> Hard Voting
				When we predict the probabilities of response variable -------> Soft Voting

        2. Bagging:
            - Classification:
                1) Creats multiple subsets of the data and apply different algorithms on every subset of the data.(rows can be duplicated from one subset to another).
                2) It takes the predicted value of all the algorithms and consider final result as the majority value among all the predicted values of the algorithm.
                    eg. Output of : 
                        Logistic Regression --> 0
                        KNN Classifier --> 0
                        DecisionTreeClassifier --> 1
                        GaussianNB --> 0
                    Final predicted value = 0
            - Regression:
                1) Calculate the MEAN of all predicted values obtained by applying multiple algorithms on dataset.
                    eg. RandomForest Classifier & Regression 
            - In bagging, the records which does not get involve into the sample acts as the testing data for that specific sample.
            
        3. Boosting:
            - Initially, we give train test data to 1st prdictive model.
            - It calculates the error in the predicted values with actual testing data.
            - Afterwards, we give the data to next model with train and test data.
            - There, We consider test data as the errors obtained in the previous model.
            - Hence, the purpose of the technique is to make a weak learning algorithm into strong one.
            - It is sequential technique.
            - eg. AdaBoost, GradientBoost, XGBoost


    -> RandomForestClassifier:
        - It is a bagging technique algorithm.
        - The drawback of DecisionTree is overfitting. It splits the column upto the complete depth and upto the extent of the feature.
        - It is collection of multiple DecisionTrees.
        - We can set 'MAX_FEATURE = value' hyperparameter. It will take only 'value' no. of columns for every split.  
            eg. Consider, there are 4 DecisionTree. 4 subsets of datasets will be created on the basis of row and feature sampling & will be given as input to DecisionTree.
                Hence, the majority value of among all 4 algorithms will be the final predicted value.
        - Standardization is not required.
        - Does not get affected by outliers.

    -> AdaBoost:
        - We apply a weight to every record of the data such that the sum of all the records will be 1.
        - After, we create a decision tree of only 1 depth by spliting the column on the basis of information gain. It is called as Stump.
        - In the 2nd iteration, the weights of errors get increased because these records must go to the next iteration and perform the next iteration.