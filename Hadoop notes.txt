> Apache Hadoop is a collection of open-source software utilities that facilitates using a network of many computers to solve problems involving massive amounts of data and computation.

> Hadoop is made up of 3 parts:
	1. HDFS(Storage) - Hadoop Distributed File System
	2. MapReduce(Processing) - processing large data sets in scalable & parallel fashion. Tool to perform ETL Operations.
	3. YARN (Cluster Resource Management) - framework for job scheduling and cluster resource management. Helps MapReduce to find where the blocks are residing.

> Hive:
	- Use to apply schema to the data when there is requirement to analyze it.

> Data Warehouse:
	- Provides a common interface to the heterogenous form of data.
	- Data warehouse data is read-only & non-volatile.

> Data Warehouse is not a cost-effective way of storing Data. Hence, Big Data tools are used to save money and store large amount of data.

>  ETL(Extract, Transform, and Load) will perform cleaning operation on the data in Staging Area.

> Hadoop Cluster:
	NameNode: Resource Manager	  ------> Main Node that manages DataNodes
		- DataNode1
		- DataNode2
		- DataNode3
		- DataNode4
		
> Port No. --->	Details
	1. 8088 --> Shows Resource Manager Details
	2. 50070 --> Shows NameNode Details
	3. 19888 --> Shows Jobs history of Hadoop. MapReduce jobs are listed in it.
	
# hdfs dfsadmin -report:
	- list the details & nodes in a hadoop cluster.
	
> NameNode = Master Process in hadoop
> DataNode = Worker Process 

> Volume: Defined as the Amount of Data Generated from the source.
> Variety: Defined as the Type of Data Generated from the source.
> Velocity: Defined as the Rate at which Data Generated from the source. eg. GB/s, MB/s, etc.

> dfs.blocksize:
	- When client sends the data to NameNode, it breaks the data into blocks(default size =128 MB) & the distributed blocks are stored in the Data Node individually.
	eg. 512 MB of data = 128 MB
						 128 MB
						 128 MB
						 128 MB
	
> dfs.replication: 
	- Creates the replication (default value = 3) of blocks to other Nodes in Cluster.
		eg. 4 blocks * 3 copies = 12 blocks.
		
> MapReduce:
	-> Mapper function:
		- Maps the data to respective blocks.

> From a DB, Java can extract limited amount of data at a time in KBs or MBs because the bandwidth of Network between Java & DB is less.
	
		DB ----------------------> Java 
				(Bandwidth)
				
> Mappers runs on the Nodes where the user requested blocks resides.

> MapReduce generates the data in Key-Value format.

> Hadoop process the data locally(on local machine) to reduce purchase and support price.

> Hadoop cannot collect the data from sources, it just stores the data. Hence, Hadoop Ecosystem Projects contains tools that collaborate with Hadoop to extract data.

> Hadoop Ecosystem Projects intergrates with Hadoop in order to collect data and store in Hadoop.
	eg. Collect opinions of people on a film from Twitter using Apache Flume (tool for streaming data in Hadoop) and store in HDFS(Hadoop).
	
> Hadoop 2.x can be used as a Data Lake. 


> Workflow of Hadoop:
	Injesting the data ---------> Clean the data-------------> Transform -----------> Store in Hadoop

> Hadoop Ecosystem Projects:
	> PIG: ETL tool used over Hadoop.
	> HIVE: Retrive results in schema format. Perform Data Analytics. 
	> HBase: Non Relation Database having depedency of Hadoop.
	> Accumulo: More secure than HBase.
	> Ambari: Create and Manage Hadoop Clusters.
	> Sqoop: Import a schema based data from source to Hadoop.
	> Falcon: Data Life Cycle Management tool of Hadoop.
	> Oozie: Creating, Schdule or Automate the WOrkflow.(used before Airflow)
	> Airflow: WOrkflow Management System.
	> Solr: Quick Searching of Data on Hadoop.
	> Storm: Capturing and Processing of Realtime data. (Realtime processing - Captures the data, process it, store it after processing immedietly). 
	> Flume: Only Capturing the Data from sources.
	> Zookeeper: Distributed coordination service.
	> Mahout: ML library in Java.
	> Spark: Do all the things PIG, HIVE, STORM, MAHOUT can do.

> Distros available with Hadoop: Helps to improve productivity for Developer.
	- Cloudera
	- HortonWorks
	- Mappa
	- AWS
	- Azure
	
> Path of ROI:
	Raw Data ----------> HDFS --------------> Perform ETL (PIG) ----------> Structured Data ---------->Data analysis (HIVE, SPARK) --------->  Answers to questions.

> Hadoop can be deployed in Linux or Windows and On-premises and CLoud.	

> Deployment Models:
	-> Standalone Mode:
		- contains single H/W machine.
		- All Hadoop Services will run as a single JVM process.
		- Cannot use HDFS. Uses local file system. Cannot make blocks to store data.
		
	-> Psudo Distributed Mode:
		- contains single H/W machine.
		- Each Hadoop Services deamon runs its own JVM process.
		- Can use HDFS as well as local file System.
		- All distros use this Mode.
		
	-> Distributed Mode:
		- Multi system installation.
		- Each Hadoop Services deamon runs its own JVM process.
		- Uses HDFS on Local Disk.
		- USed in Production environment.

> HDFS(Hadoop Distributed File System):
	- Difference betn Linux/ Windows File system and HDFS is that, HDFS is distributed file system and Linux/ Windows resides on 1 system.
	- Automatically maintains multiple copies of data block.
	- Write-once, read-many times.
	
> NameNode is used to manage Namespace and Metadata (Permissions, Directory names, file system hierarcy, logs, etc.) of the DataNode.

> In order to access any data from DataNode, the request 1st goes to the NameNode and then gets routed to respective DataNode.

> Linux does not create block on File System. Hence, HDFS is an application that works on Linux on DataNode which creates the blocks on the storage and Linux helps to stores the data into the blocks.

> DataNode holds the actual Data.

> Creating a replica of blocks is responsibilty of DataNode.

> Journaling Commands: Commands which change the state of a file.
						eg. touch- adding a new file under a folder, changes the state of folder.
						
> Checkpointing: Stores In memory state of files.

> Secondary NameNode can retrive the state of files, in case NameNode gets crashed, by checkpointing. Responsible for Checkpointing. Records the changes made in fs_image and merges it with edits.

> Writing File to HDFS: 
		Client node --------------------------> NameNode --------------------------------> DataNode1
																						   DataNode2
																						   DataNode3
																						   
> Checksum is used to maintain the integrity of data.
	eg. DataNode1 must contain the same data(unchanged) like DataNode2.
	
> 1st replica of actual data is made on a different rack of DataNodes and 2nd replica is also made on different machine of same rack because the interrack communication of data is costlier rather than communicating with machines on same rack.

> Home folder for hdfs user is /user/<distro_name>                 
	eg. (/user/cloudera)
	
> Commands for Development environment and production environment is same.

> fsimage : Holds Metadata and Namespace. Store state of NameNode
> edits: store journaling information.


# sudo find / -type f -name <filename.ext> -----> Find a file in linux

> NameNode will be in safe mode (read-only) when fs_image and edits files are read during startup.

> 'dfs' acts like a bash of HDFS.

> 3 types of Commands in HDFS:
	1. User (hdfs dfs): Manage files, directories and metadata. 
	2. Inconsistency (fsck): check and reports on file system inconsistencies(does not repair).
	3. Admin (hdfs dfsadmin): Reports basic file system information and statistics.
	
# put - Use to copy Local file into HDFS.
# get - Retrive files in HDFS into local machine.
# hdfs dfs -du [-s] [-h] [path]: (Disk Usage): shows no. of bytes consumed by a file or directory.
# hdfs dfs -df : Shows Total Capacity of Disk and disk space consumed.
# sudo netstat -nputl [| grep <process_id>]: Gives stats of all process_ids (display ports where the process is running).

> "core-site.xml" will contain all the information about the port through which EdgeNode and NameNode can listen the request. They are also present on all DataNode so that they can communicate with NameNode.

> Fat Client: All the libraries are installed in the local machine of the client. Uses RPC post 8020
> Thin Client: Uses HTTP port i.e. 50070/50075

--------------------------------------------------------------------------------------------------------------------------------------------

> Sqoop:
	-> '-m' is use to indicate no. of mappers used to pull the data. Deafult value is 4.
	-> 'split-by' distributes the data into 4 equal parts. eg. 1000 records will be divided into 250 each and send to the mappers.
	-> By default, 'split-by' splits the table on the basis of Primary key.
	-> The query i.e whole data is distributed among the 4 mappers to store the data.
		eg.	-rw-r--r--   1 cloudera cloudera          0 2023-05-25 02:37 salaries/_SUCCESS
			-rw-r--r--   1 cloudera cloudera        272 2023-05-25 02:37 salaries/part-m-00000
			-rw-r--r--   1 cloudera cloudera        241 2023-05-25 02:37 salaries/part-m-00001
			-rw-r--r--   1 cloudera cloudera        238 2023-05-25 02:37 salaries/part-m-00002
			-rw-r--r--   1 cloudera cloudera        272 2023-05-25 02:37 salaries/part-m-00003
			
			- salaries.txt is uploaded and by default it is stored in salaries folder. Above are the 4 Mappers used to import the data in HDFS.

	-> If no. of mappers exceeds the limit w.r.to data, the threshold value decreases and performance starts diminishing.
	-> If folder is not specified on HDFS, by default table name is taken as folder name.
	-> Mapper history can be viewed in localhost:19888/JobHistory, click on job id and click on logs by clicking on 'Attempt Type'. 

> 3 ways to see logs:
	1. Job History: 19888
	2. localhost:8088
	3. using 'yarn'

> inserting values into Hadoop with sqoop:
	# sqoop import --connect jdbc:mysql://quickstart.cloudera:3306/test --driver com.mysql.jdbc.Driver --username root -password cloudera --table <table_name>

> select Query using sqoop
# 	sqoop import --connect jdbc:mysql://quickstart.cloudera:3306/test --driver com.mysql.jdbc.Driver --username root --password cloudera 
	--query "select * from salaries s where s.salary > 90000.00 and \$CONDITIONS" --split-by gender -m 2 --target-dir salaries3 
	
	-> --query cannot be used without --split-by.
	-> \$CONDITIONS 

> to display LOGS of a job in terminal:
#	yarn logs -applicationId <application_jobid> 									
		- eg. yarn logs -applicationId application_1685068466243_0003
		
> Distribution Copy(DistCP):
	- Create a replica of the data from one cluster into another Hadoop cluster.(Just like AWS replica) to avoid data loss.
	- Uses MapReduce mappers to make the copy of the data.
	- if mapps not specified, Default amppers will be 20.
	# hadoop distcp hdfs://<namenode1>/<source_folder_path>  hdfs://<namenode2>:<portno>/<destination_folder_path>
	
> Try automating the above process.
--------------------------------------------------------------------------------------------------------------------------------------------
> MapReduce:
	- Mappers: Responsible for collecting data from source.
	- Reduce: Contains business logic of processing the data(How to process the data).
	- Shuffle/Sort: Shuffle the data generated by Map and sends it to Reduce. It is handled by Framework.
	- Stores the data in <key,value> pair.
	- After collecting data by Mappers, Shuffle/Sort will shuffle and sort the data where all the data with same key is send to one Reducer. The data will be sent in the form of single key and list of values(<key1,[value1,value2,value3,....]>).
	- Reduce gives the output in <key,value> pair.
	
	Mapper ------------------------------> Shuffle/Sort ------------------------------------> Reducer
				<K1,V1>						<K2,V2>						<K3,V3>
				


# yarn jar /usr/hdp/2.6.0.3-8/hadoop-mapreduce/hadoop-mapreduce-examples.jar wordcount constitution.txt wordcount_output -D mapred.reduce.tasks=2

> InputFormat is responsible for generating keys and value pair <K1,V1> and give it to Mapper.

> K1 is the key for every new line in the file.

> Context refers to a memory output buffer where <K2,V2> is written.

> WritableComparable: used to sort the values and write it on the dist of DataNode.

> YARN (Yet Another Resource Negotiator): resposible for launching the Mappers and Reducers on HDFS.

> Container: Memory of a DataNode.

> Only associative operations(eg.a+b=b+a) can only be performed by the Combiner.

> If a container fails to complete its task in YARN application, by deafult it will make 3 attempts on 3 replicas to retrieve the data. Yet it fails to retrieve, it will mark the job as fail.

> Combiner is used to reduce the network traffic to the reducer. Combiner is a local reducer. (Reducer performs global reduction).

> Counters are the debugger of the application.

> Partitioner takes care of which record must be send to which reducer on the basis of Hashcode generated by the key.(eg. Records with same key must go to 1 reducer).

> If we do not declare a partitioner in the code, the default partitioner named "HashPartitioner" is called implicitly.

> Partitioner will work only if the number of given reducers is more than 1.

> 