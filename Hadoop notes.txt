> Apache Hadoop is a collection of open-source software utilities that facilitates using a network of many computers to solve problems involving massive amounts of data and computation.

> 3 V's of Big Data:
	> Volume: Defined as the Amount of Data Generated from the source.
	> Variety: Defined as the Type of Data Generated from the source.
	> Velocity: Defined as the Rate at which Data Generated from the source. eg. GB/s, MB/s, etc.

> Data Warehouse:
	- Provides a common interface to the heterogeneous form of data.
	- Data warehouse data is read-only & non-volatile.

> A Data Warehouse is not a cost-effective way of storing Data. Hence, Big Data tools are used to save money and store large amounts of data.

> Components of Hadoop (3 Parts):
	1. HDFS(Storage) - Hadoop Distributed File System. Uses the Google File System to store the data. Break file into chunks and stores them in the distributed nodes. 
	2. MapReduce(Processing) - processing large data sets in scalable & parallel fashion. Tool to perform ETL Operations.
	3. YARN (Cluster Resource Management) - framework for job scheduling and cluster resource management. Helps MapReduce to find where the blocks are residing.


> Hadoop Common: Java Libraries used to start hadoop and other Hadoop modules.

> Hadoop Architecture consists of:
	- File System
	- MapReduce engine
		- MapReduce / MR1
		- YARN / MR2
	- HDFS

> Hadoop consists of single master and multiple slave nodes.

> Master node (NameNode) consists of:
	- Job Tracker: 
	- Task Tracker
	- NameNode
	- DataNode

>  Slave Node (DataNode) consists of:
	- DataNode
	- Task Tracker

>  ETL(Extract, Transform, and Load) will perform cleaning operations on the data in the Staging Area.

> Hadoop Cluster:
	NameNode: Resource Manager	  ------> Main Node that manages DataNodes
		- DataNode1
		- DataNode2
		- DataNode3
		- DataNode4
		
> Port No. --->	Details
	1. 8088 --> Shows Resource Manager Details
	2. 50070 --> Shows NameNode Details
	3. 19888 --> Shows Jobs history of Hadoop. MapReduce jobs are listed in it.
	
# hdfs dfsadmin -report:
	- list the details & nodes in a hadoop cluster.
	
> NameNode:
	- Single POC for the client to contact with the cluster.
	- Master Process in hadoop. 
	- Manages file system namespace by executing operations like opening, renaming, and closing files.  
	
> DataNode:
	- Nodes that hold the actual data.
	- Worker Process.
	- Responsible for reading and writing requests from the file system.
	- Performs block creation, deletion, and replication of data.

> Job Tracker:
	- Accept MapReduce jobs from the client and process data using NameNode.
	- Hold the info. about which node is executing which job. 
	- NameNode provides metadata (details of job) to the Job Tracker.

> Task Tracker:
	- Slave Node for Job Tracker.
	- Used to map the data and metadata of Job Tracker.
	- Receives task and code from Job Tracker and applies that code on file which is called Mapper.

> Client application sends the job to Job Tracker ---> Job Tracker sends the request to Task Tracker and it executes the job.

> In case, Task Tacker fails, the job gets rescheduled.

> dfs.blocksize:
	- When client sends the data to NameNode, it breaks the data into blocks(default size =128 MB) & the distributed blocks are stored in the Data Node individually.
	eg. 512 MB of data = 128 MB
						 128 MB
						 128 MB
						 128 MB
	
> dfs.replication: 
	- Creates the replication (default value = 3) of blocks to other Nodes in Cluster.
		eg. 4 blocks * 3 copies = 12 blocks.
		
> MapReduce:
	-> Mapper function:
		- Maps the data to respective blocks.

> From a DB, Java can extract limited amount of data at a time in KBs or MBs because the bandwidth of Network between Java & DB is less.
	
		DB ----------------------> Java 
				(Bandwidth)
				
> Mappers runs on the Nodes where the user requested blocks resides.

> MapReduce generates the data in Key-Value format.

> Hadoop process the data locally(on local machine) to reduce purchase and support price.

> Hadoop cannot collect the data from sources, it just stores the data. Hence, Hadoop Ecosystem Projects contains tools that collaborate with Hadoop to extract data.

> Hadoop Ecosystem Projects intergrates with Hadoop in order to collect data and store in Hadoop.
	eg. Collect opinions of people on a film from Twitter using Apache Flume (tool for streaming data in Hadoop) and store in HDFS(Hadoop).
	
> Hadoop 2.x can be used as a Data Lake. 

> Workflow of Hadoop:
	Injesting the data ---------> Clean the data-------------> Transform -----------> Store in Hadoop

> Hadoop Ecosystem Projects:
	> PIG: ETL tool used over Hadoop.
	> HIVE: Retrive results in schema format. Perform Data Analytics. Use to apply schema to the data when there is a requirement to analyze it.
	> HBase: Non Relation Database having depedency of Hadoop.
	> Accumulo: More secure than HBase.
	> Ambari: Create and Manage Hadoop Clusters.
	> Sqoop: Import a schema based data from source to Hadoop.
	> Falcon: Data Life Cycle Management tool of Hadoop.
	> Oozie: Creating, Schdule or Automate the WOrkflow.(used before Airflow)
	> Airflow: WOrkflow Management System.
	> Solr: Quick Searching of Data on Hadoop.
	> Storm: Capturing and Processing of Realtime data. (Realtime processing - Captures the data, process it, store it after processing immedietly). 
	> Flume: Only Capturing the Data from sources.
	> Zookeeper: Distributed coordination service.
	> Mahout: ML library in Java.
	> Spark: Do all the things PIG, HIVE, STORM, MAHOUT can do.

> Distros available with Hadoop: Helps to improve productivity for Developer.
	- Cloudera
	- HortonWorks
	- Mappa
	- AWS
	- Azure
	
> Path of ROI:
	Raw Data ----------> HDFS --------------> Perform ETL (PIG) ----------> Structured Data ---------->Data analysis (HIVE, SPARK) --------->  Answers to questions.

> Hadoop can be deployed in Linux or Windows and On-premises and CLoud.	

> Deployment Models:
	-> Standalone Mode:
		- contains single H/W machine.
		- All Hadoop Services will run as a single JVM process.
		- Cannot use HDFS. Uses local file system. Cannot make blocks to store data.
		
	-> Psudo Distributed Mode:
		- contains single H/W machine.
		- Each Hadoop Services deamon runs its own JVM process.
		- Can use HDFS as well as local file System.
		- All distros use this Mode.
		
	-> Distributed Mode:
		- Multi system installation.
		- Each Hadoop Services deamon runs its own JVM process.
		- Uses HDFS on Local Disk.
		- USed in Production environment.

> HDFS(Hadoop Distributed File System):
	- Difference betn Linux/ Windows File system and HDFS is that, HDFS is distributed file system and Linux/ Windows resides on 1 system.
	- Automatically maintains multiple copies of data block.
	- Write-once, read-many times.
	
> NameNode is used to manage Namespace and Metadata (Permissions, Directory names, file system hierarcy, logs, etc.) of the DataNode.

> In order to access any data from DataNode, the request 1st goes to the NameNode and then gets routed to respective DataNode.

> Linux does not create block on File System. Hence, HDFS is an application that works on Linux on DataNode which creates the blocks on the storage and Linux helps to stores the data into the blocks.

> DataNode holds the actual Data.

> Creating a replica of blocks is responsibilty of DataNode.

> Journaling Commands: Commands which change the state of a file.
						eg. touch- adding a new file under a folder, changes the state of folder.
						
> Checkpointing: Stores In memory state of files.

> Secondary NameNode can retrive the state of files, in case NameNode gets crashed, by checkpointing. Responsible for Checkpointing. Records the changes made in fs_image and merges it with edits.

> Writing File to HDFS: 
		Client node --------------------------> NameNode --------------------------------> DataNode1
																						   DataNode2
																						   DataNode3
																						   
> Checksum is used to maintain the integrity of data.
	eg. DataNode1 must contain the same data(unchanged) like DataNode2.
	
> 1st replica of actual data is made on a different rack of DataNodes and 2nd replica is also made on different machine of same rack because the interrack communication of data is costlier rather than communicating with machines on same rack.

> Home folder for hdfs user is /user/<distro_name>                 
	eg. (/user/cloudera)
	
> Commands for Development environment and production environment is same.

> fsimage : Holds Metadata and Namespace. Store state of NameNode
> edits: store journaling information.


# sudo find / -type f -name <filename.ext> -----> Find a file in linux

> NameNode will be in safe mode (read-only) when fs_image and edits files are read during startup.

> 'dfs' acts like a bash of HDFS.

> 3 types of Commands in HDFS:
	1. User (hdfs dfs): Manage files, directories and metadata. 
	2. Inconsistency (fsck): check and reports on file system inconsistencies(does not repair).
	3. Admin (hdfs dfsadmin): Reports basic file system information and statistics.
	
# put - Use to copy Local file into HDFS.
# get - Retrive files in HDFS into local machine.
# hdfs dfs -du [-s] [-h] [path]: (Disk Usage): shows no. of bytes consumed by a file or directory.
# hdfs dfs -df : Shows Total Capacity of Disk and disk space consumed.
# sudo netstat -nputl [| grep <process_id>]: Gives stats of all process_ids (display ports where the process is running).

> "core-site.xml" will contain all the information about the port through which EdgeNode and NameNode can listen the request. They are also present on all DataNode so that they can communicate with NameNode.

> Fat Client: All the libraries are installed in the local machine of the client. Uses RPC post 8020
> Thin Client: Uses HTTP port i.e. 50070/50075

--------------------------------------------------------------------------------------------------------------------------------------------

> Sqoop:
	-> '-m' is use to indicate no. of mappers used to pull the data. Deafult value is 4.
	-> 'split-by' distributes the data into 4 equal parts. eg. 1000 records will be divided into 250 each and send to the mappers.
	-> By default, 'split-by' splits the table on the basis of Primary key.
	-> The query i.e whole data is distributed among the 4 mappers to store the data.
		eg.	-rw-r--r--   1 cloudera cloudera          0 2023-05-25 02:37 salaries/_SUCCESS
			-rw-r--r--   1 cloudera cloudera        272 2023-05-25 02:37 salaries/part-m-00000
			-rw-r--r--   1 cloudera cloudera        241 2023-05-25 02:37 salaries/part-m-00001
			-rw-r--r--   1 cloudera cloudera        238 2023-05-25 02:37 salaries/part-m-00002
			-rw-r--r--   1 cloudera cloudera        272 2023-05-25 02:37 salaries/part-m-00003
			
			- salaries.txt is uploaded and by default it is stored in salaries folder. Above are the 4 Mappers used to import the data in HDFS.

	-> If no. of mappers exceeds the limit w.r.to data, the threshold value decreases and performance starts diminishing.
	-> If folder is not specified on HDFS, by default table name is taken as folder name.
	-> Mapper history can be viewed in localhost:19888/JobHistory, click on job id and click on logs by clicking on 'Attempt Type'. 

> 3 ways to see logs:
	1. Job History: 19888
	2. localhost:8088
	3. using 'yarn'

> inserting values into Hadoop with sqoop:
	# sqoop import --connect jdbc:mysql://quickstart.cloudera:3306/test --driver com.mysql.jdbc.Driver --username root -password cloudera --table <table_name>

> select Query using sqoop
# 	sqoop import --connect jdbc:mysql://quickstart.cloudera:3306/test --driver com.mysql.jdbc.Driver --username root --password cloudera 
	--query "select * from salaries s where s.salary > 90000.00 and \$CONDITIONS" --split-by gender -m 2 --target-dir salaries3 
	
	-> --query cannot be used without --split-by.
	-> \$CONDITIONS 

> to display LOGS of a job in terminal:
#	yarn logs -applicationId <application_jobid> 									
		- eg. yarn logs -applicationId application_1685068466243_0003
		
> Distribution Copy(DistCP):
	- Create a replica of the data from one cluster into another Hadoop cluster.(Just like AWS replica) to avoid data loss.
	- Uses MapReduce mappers to make the copy of the data.
	- if mapps not specified, Default amppers will be 20.
	# hadoop distcp hdfs://<namenode1>/<source_folder_path>  hdfs://<namenode2>:<portno>/<destination_folder_path>
	
> Try automating the above process.
--------------------------------------------------------------------------------------------------------------------------------------------
