> MapReduce:
	- Mappers: Responsible for collecting data from source.
	- Reduce: Contains business logic of processing the data(How to process the data).
	- Shuffle/Sort: Shuffle the data generated by Map and sends it to Reduce. It is handled by Framework.
	- Stores the data in <key,value> pair.
	- After collecting data by Mappers, Shuffle/Sort will shuffle and sort the data where all the data with same key is send to one Reducer. The data will be sent in the form of single key and list of values(<key1,[value1,value2,value3,....]>).
	- Reduce gives the output in <key,value> pair.
	
	Mapper ------------------------------> Shuffle/Sort ------------------------------------> Reducer
				<K1,V1>						<K2,V2>						<K3,V3>
				


# yarn jar /usr/hdp/2.6.0.3-8/hadoop-mapreduce/hadoop-mapreduce-examples.jar wordcount constitution.txt wordcount_output -D mapred.reduce.tasks=2

> InputFormat is responsible for generating keys and value pair <K1,V1> and give it to Mapper.

> K1 is the key for every new line in the file.

> Context refers to a memory output buffer where <K2,V2> is written.

> WritableComparable: used to sort the values and write it on the dist of DataNode.

> YARN (Yet Another Resource Negotiator): resposible for launching the Mappers and Reducers on HDFS.

> Container: Memory of a DataNode.

> Only associative operations(eg.a+b=b+a) can only be performed by the Combiner.

> If a container fails to complete its task in YARN application, by deafult it will make 3 attempts on 3 replicas to retrieve the data. Yet it fails to retrieve, it will mark the job as fail.

> Combiner is used to reduce the network traffic to the reducer. Combiner is a local reducer. (Reducer performs global reduction).

> Counters are the debugger of the application.

> Partitioner takes care of which record must be send to which reducer on the basis of Hashcode generated by the key.(eg. Records with same key must go to 1 reducer).

> If we do not declare a partitioner in the code, the default partitioner named "HashPartitioner" is called implicitly.

> Partitioner will work only if the number of given reducers is more than 1.

> Hadoop-Streaming is convertion of output of code from any other language to Java, to provide the facility to code in any language to the developer.

> In python, the input(stdin) of Mapper(python prog of Mapper)and Reducer(python prog of Reducer) is taken in form of lines and output(stdout) is converted into Java objects by the Framework.

> In python, the stdout of Mapper is generated in form of Java Objects and given as Lines to the stdin of Reducer.
----------------------------------------------------------------------------------------------------------------------------